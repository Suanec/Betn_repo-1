<configuration>
  <weiflow>
    <node id="1" preid="-1">GenDataConf</node>
    <node id="2" preid="-1">GenFeatureConf</node>
    <node id="3" preid="-1">GenSparseLibsvm</node>
    <node id="4" preid="-1">TrainingTronLR</node>
    <node id="5" preid="-1">Test</node>
    <node id="6" preid="-1">Predict</node>
    <node id="7" preid="-1">TrainW2V</node>
    <node id="8" preid="-1">findSynonyms</node>
    <node id="9" preid="-1">libsvmStatistics</node>
  </weiflow>
  <nodes>
    <node name="GenDataConf">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --num-executors 2
          --executor-cores 20
          --executor-memory 2g
          --driver-memory 20g
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input1">
        <className>com.weibo.datasys.engine.spark.input.InputSparkDataMeta</className>
        <dataPath>meta</dataPath>
      </input>
      <output name="output1">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkDataConf</className>
        <dependency>input1</dependency>
        <dataPath>data.conf.test</dataPath>
        <fieldDelimiter>;</fieldDelimiter>
      </output>
    </node>
    <node name="GenFeatureConf">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --num-executors 2
          --executor-cores 20
          --executor-memory 8g
          --driver-memory 10g
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input2">
        <className>com.weibo.datasys.engine.spark.input.InputSparkDataConf</className>
        <dataPath>data.conf</dataPath>
      </input>
      <output name="output2">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkFeatureConf</className>
        <dependency>input2</dependency>
        <dataPath>feature.conf</dataPath>
      </output>
    </node>
    <node name="GenSparseLibsvm">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --files meta,data.conf,feature.conf,pipeline.xml,dataflow.jar
          --num-executors 7
          --executor-cores 10
          --executor-memory 20g
          --driver-memory 20g
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input3">
        <className>com.weibo.datasys.engine.spark.input.InputSparkText</className>
        <dataPath>/user/suanec/dataflow/littleSample/1w</dataPath>
        <metaPath>meta</metaPath>
        <fieldDelimiter>\t</fieldDelimiter>
      </input>
      <process name="process3-1">
        <className>com.weibo.datasys.engine.spark.process.ProcessSparkDataClean</className>
        <dependency>input3</dependency>
        <dataPath>data.conf</dataPath>
      </process>
      <process name="process3-2">
        <className>com.weibo.datasys.engine.spark.process.ProcessSparkDataExtract</className>
        <dependency>process3-1</dependency>
        <dataPath>data.conf</dataPath>
      </process>
      <process name="process3-3">
        <className>com.weibo.datasys.engine.spark.process.ProcessSparkFeatureMapping</className>
        <dependency>process3-2</dependency>
        <dataPath>feature.conf</dataPath>
      </process>
      <output name="output3">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkLibsvm</className>
        <dependency>process3-3</dependency>
        <dataPath>/user/suanec/dataflow/libsvm/testSample/1w-parquet</dataPath>
        <format>parquet</format>
      </output>
    </node>
    <node name="TrainingTronLR">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --executor-cores 4
          --num-executors 60
          --driver-memory 25G
          --executor-memory 20G
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input4">
        <className>com.weibo.datasys.engine.spark.input.InputSparkLibsvm</className>
        <textdataPath>/user/suanec/dataflow/libsvm/testSample/1w-parquet</textdataPath>
        <pdataPath>/user/suanec/dataflow/libsvm/testSample/1w-parquet</pdataPath>
        <dataPath>/user/suanec/dataflow/libsvm/testSample/1w-txt</dataPath>
        <tformat>textWithBzip</tformat>
        <pformat>parquet</pformat>
        <format>text</format>
      </input>
      <process name="process4-1">
        <className>com.weibo.datasys.engine.spark.process.ProcessSparkLibsvmStatistics</className>
        <dependency>input4</dependency>
        <checkIllegalLabel>true</checkIllegalLabel>
        <rateNP>true</rateNP>
        <illegalFeatrues>true</illegalFeatrues>
        <featureNum>true</featureNum>
        <sparsityCategory>allSparsity</sparsityCategory>
        <featureDistribution>true</featureDistribution>
        <statusPath>/home/suanec/ksp/dataflow/model/libsvmStatistics/libsvm-status.0731-1</statusPath>
      </process>
      <output name="output4">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkWeiboLR.Train</className>
        <dependency>process4-1</dependency>
        <dataPath>/user/feed_weibo/a4a/tests_data/parquet/train</dataPath>
	<haveIntercept>false</haveIntercept>
        <modelVersion>20170710-2</modelVersion>
        <modelPath>/home/suanec/ksp/dataflow/model/tron-mllib.0725-1</modelPath>
	<regType>L2</regType>
	<regParam>1.0</regParam>
	<learningRateOrigin>1e-4</learningRateOrigin>
	<learningRateConvTol>1e-12</learningRateConvTol>
	<convTol>1e-12</convTol>
	<maxIter>1</maxIter>
	<numFeatures>240</numFeatures>
	<featureSize>240</featureSize>
      </output>
    </node>
    <node name="Test">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --executor-cores 4
          --num-executors 60
          --driver-memory 25G
          --executor-memory 20G
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input5">
        <className>com.weibo.datasys.engine.spark.input.InputSparkLibsvm</className>
        <dataPath>/user/feed_weibo/a4a/tests_data/parquet/test</dataPath>
        <format>parquet</format>
      </input>
      <output name="output5">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkWeiboLR.Test</className>
        <dependency>input5</dependency>
        <modelPath>/home/suanec/ksp/dataflow/model/tron-mllib.0710-2</modelPath>
        <DataFormat>parquet</DataFormat>
        <FeatureSize>240</FeatureSize>
        <dataPath>/user/feed_weibo/a4a/tests_data/parquet/test</dataPath>
      </output>
    </node>
    <node name="Predict">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --executor-cores 4
          --num-executors 10
          --driver-memory 20G
          --executor-memory 15G
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input6">
        <className>com.weibo.datasys.engine.spark.input.InputSparkLibsvm</className>
        <dataPath>/user/feed_weibo/a4a/tests_data/parquet/test</dataPath>
        <format>parquet</format>
      </input>
      <output name="output6">
        <className>com.weibo.datasys.engine.spark.output.OutputSparkWeiboLR.Predict</className>
        <dependency>input6</dependency>
        <modelPath>/home/suanec/ksp/dataflow/model/tronLR/tron-mllib.0710-2</modelPath>
	<outputPath>/user/feed_weibo/a4a/tests_data/parquet/predict-0719</outputPath>
        <DataFormat>parquet</DataFormat>
        <Eta>0.0000001</Eta>
        <IterationTimes>1</IterationTimes>
        <BatchSize>1000</BatchSize>
        <FeatureSize>60000</FeatureSize>
        <GlintMasterHost>emr-worker-179.cluster-40569</GlintMasterHost>
        <GlintTimeOut>600</GlintTimeOut>
        <GlintServerCount>100</GlintServerCount>
        <dataPath>/user/feed_weibo/a4a/tests_data/parquet/predict-0710</dataPath>
      </output>
    </node>
  <node name="TrainW2V">
    <!-- 运行环境配置 -->
    <env>
      <var>SPARK_HOME=/usr/local/spark2.1.0</var>
      <var>SCALA_HOME=/usr/local/scala2.11.5</var>
    </env>
    <!-- 运行时配置，包括DAGNode参数和engine for spark参数配置 -->
    <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
      <binPath>/usr/local/spark/bin/spark-submit</binPath>
      <sparkArgs>
        --master yarn
        --deploy-mode client
        --executor-cores 4
        --num-executors 60
        --driver-memory 25G
        --executor-memory 20G
      </sparkArgs>
    </runtime>
    <execute>
      <type>pipeline</type>
    </execute>
    <!-- 输入node节点，作为w2v模型训练节点依赖节点，进行数据读入 -->
    <input name="input7">
      <!-- 读入语料数据className -->
      <className>com.weibo.datasys.engine.spark.input.InputSparkWord2Vec.InputSparkCorpus</className>
      <!-- 预料数据存储路径，默认由hdfs读入 -->
      <dataPath>/user/suanec/dataflow/w2v/cut_words</dataPath>
      <!-- 数据读入格式，默认text -->
      <format>text</format>
    </input>
    <output name="output7">
      <!-- Word2Vec模型训练className -->
      <className>com.weibo.datasys.engine.spark.output.OutputSparkWord2Vec.Train</className>
      <!-- 依赖节点，读入数据由依赖节点提供 -->
      <dependency>input7</dependency>
      <!-- 数据路径，w2v中此项不起作用，为将来需要独立配置文件预留,但需要配置，可为空 -->
      <dataPath>/user/suanec/dataflow/w2v/cut_words</dataPath>
      <!--  -->
      <!-- 模型训练参数 begin -->
      <!-- stepSize，必须项 -->
      <stepSize>0.025</stepSize>
      <!-- window，必须项 -->
      <window>5</window>
      <!-- vectorSize，必须项 -->
      <vectorSize>10</vectorSize>
      <!-- numIterations，必须项 -->
      <numIterations>10</numIterations>
      <!-- minCoun，非必须项 -->
      <minCount>10</minCount>
      <!-- maxSentenceLength，必须项 -->
      <maxSentenceLength>5</maxSentenceLength>
      <!-- numPartitions，必须项，如不配置，将自动由数据分片情况动态读取 -->
      <numPartitions>0</numPartitions>
      <!-- seed，非必须项，如不配置，将会有系统自动配置随机种子并分发，默认类型Long -->
      <seed>0</seed>
      <!-- 模型训练参数 end -->
      <!--  -->
      <!-- readable，非必须项，为随后若模型需要上线，重新定义模型文件协议的情况预留。默认为false -->
      <readable>true</readable>
      <!-- modelVersion，非必须项，为随后若模型需要上线，重新定义模型文件协议的情况预留。无默认值 -->
      <modelVersion>20170721-1</modelVersion>
      <!-- modelPath，模型文件输出路径，默认写入hdfs，如需保存到本地，请添>加“file://”前缀，必须项 -->
      <modelPath>/home/suanec/ksp/dataflow/model/w2v/20170721-1</modelPath>
      <!-- wordDelimiter，非必须项，word与vector间分隔符，默认为制表符"\t"-->
      <wordDelimiter>\t</wordDelimiter>
      <!-- vectorDelimiter,非必须项，vector间分隔符，默认为竖线"|"-->
      <vectorDelimiter>|</vectorDelimiter>
    </output>
  </node>
    <node name="findSynonyms">
        <!-- 运行环境配置 -->
        <env>
            <var>SPARK_HOME=/usr/local/spark2.1.0</var>
            <var>SCALA_HOME=/usr/local/scala2.11.5</var>
        </env>
        <!-- 运行时配置，包括DAGNode参数和engine for spark参数配置 -->
        <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
            <binPath>/usr/local/spark/bin/spark-submit</binPath>
            <sparkArgs>
                --master yarn
                --deploy-mode client
                --executor-cores 4
                --files /home/suanec/ksp/dataflow/model/w2v/20170721-1
                --num-executors 60
                --driver-memory 25G
                --executor-memory 20G
            </sparkArgs>
        </runtime>
        <execute>
            <type>pipeline</type>
        </execute>
        <input name="input8">
            <!-- 读入语料数据className -->
            <className>com.weibo.datasys.engine.spark.input.InputSparkWord2Vec.InputSparkW2VModel</className>
            <!-- modelPath，必填项，模型文件储存路径，为保证集群各计算节点均可正确读取模型文件，请将文件置于分布式储存上。 -->
            <!-- 不接受本地文件 -->
            <dataPath>file:///home/suanec/ksp/dataflow/model/w2v/20170721-1</dataPath>
            <!-- 数据读入格式，默认text -->
            <format>text</format>
        </input>
        <output name="output8">
        <!-- Word2Vec模型训练className -->
        <className>com.weibo.datasys.engine.spark.output.OutputSparkWord2Vec.CalcSimilar</className>
        <dependency>input8</dependency>
            <!-- 数据路径，w2v中此项不起作用，为将来需要独立配置文件预留,但需要配置，可为空 -->
            <dataPath>/user/suanec/dataflow/w2v/cut_words</dataPath>
            <!--  -->
            <!-- 相似度计算参数 begin -->
            <!-- dataPath，选填项，当前版本中未使用  -->
            <dataPath></dataPath>
            <modelPath></modelPath>
            <!-- rstPath，必填项，计算结果文件储存路径 -->
            <rstPath>/user/suanec/dataflow/w2v/model/20170721-rst2</rstPath>
            <!-- readable，模型文件是否可读，默认为true，请根据训练配置结合模型文件格式使用 -->
            <readable>true</readable>
            <!-- nSimilar，寻找相似词语n值，默认为5. -->
            <nSimilar>5</nSimilar>
            <!-- threshold，相似度阈值，默认为0，0代表不配置相似度阈值 -->
            <threshold>0.63</threshold>
            <!-- wordDelimiter，非必须项，word与vector间分隔符，默认为制表符"\t" -->
            <wordDelimiter>\t</wordDelimiter>
            <!-- vectorDelimiter,非必须项，vector间分隔符，默认为竖线"|" -->
            <vectorDelimiter>|</vectorDelimiter>
            <!-- pairDelimiter，非必须项，输出结果中结果词语相似度分隔符 -->
            <pairDelimiter>@</pairDelimiter>
            <!-- 相似度计算参数 end -->
            <!--  -->
        </output>
    </node>
    <node name="libsvmStatistics">
      <env>
        <var>SPARK_HOME=/usr/local/spark2.1.0</var>
        <var>SCALA_HOME=/usr/local/scala2.11.5</var>
      </env>
      <runtime name="com.weibo.datasys.engine.spark.node.NodeSparkImpl">
        <binPath>/usr/local/spark/bin/spark-submit</binPath>
        <sparkArgs>
          --master yarn
          --deploy-mode client
          --num-executors 7
          --executor-cores 10
          --executor-memory 10g
          --driver-memory 20g
        </sparkArgs>
      </runtime>
      <execute>
        <type>pipeline</type>
      </execute>
      <input name="input9">
        <className>com.weibo.datasys.engine.spark.input.InputSparkLibsvm</className>
        <dataPath>/user/suanec/dataflow/libsvm/testSample/1w-txt</dataPath>
      </input>
       <process name="process9">
        <className>com.weibo.datasys.engine.spark.process.ProcessSparkLibsvmStatistics</className>
        <dependency>input1</dependency>
      </process>
    </node>
   </nodes>
</configuration>
