import scala.collection._
import scala.collection.mutable.HashMap

import com.weibo.datasys.engine.spark.node._
import com.weibo.datasys.engine.spark.input.InputSparkLibsvm
import com.weibo.datasys.common.filesystem.File2DataFrame
import com.weibo.datasys.dataflow.classbase.input.InputSpark
import com.weibo.datasys.engine.spark.node.{DataFlowType, WeiDataFrame}

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql._


val paramMap: HashMap[String, String] = HashMap[String, String]()
paramMap += "textDataPath" -> "/user/suanec/dataflow/libsvm/testSample/1w-txt"
paramMap += "parquetDataPath" -> "/user/suanec/dataflow/libsvm/testSample/1w-parquet"
paramMap += "format" -> "text"
paramMap += "dataPath" -> paramMap("textDataPath")


paramMap += "checkIllegalLabel" -> "true"
paramMap += "rateNP" -> "true"
paramMap += "illegalFeatrues" -> "true"
paramMap += "featureNum" -> "true"
paramMap += "sparsityCategory" -> "maxSparsity"
paramMap += "featureDistribution" -> "true"
paramMap += "statusPath" -> ""
InputSparkLibsvm.init(paramMap.toMap)
val input = InputSparkLibsvm.read(spark).asInstanceOf[WeiDataFrame].get


/// @version : 0.2
/// @data : 2016/10/24

object readLibSVM extends Serializable{

  /// single line libsvm to Sparse Vector Format
  def parseLibSVMRecord(line: String): (Double, Array[Int], Array[Double]) = {
    val items = line.split(' ')
    val label = items.head.toDouble
    val (indices, values) = items.tail.filter(_.nonEmpty).map { item =>
      val indexAndValue = item.split(':')
      val index = indexAndValue(0).toInt - 1 // Convert 1-based indices to 0-based.
    val value = indexAndValue(1).toDouble
      (index, value)
    }.unzip

    // check if indices are one-based and in ascending order
    var previous = -1
    var i = 0
    val indicesLength = indices.length
    while (i < indicesLength) {
      val current = indices(i)
      require(current > previous, s"indices should be one-based and in ascending order;"
        + " found current=$current, previous=$previous; line=\"$line\"")
      previous = current
      i += 1
    }
    (label, indices.toArray, values.toArray)
  }

  // get the libsvm dimision(the max indices)
  def computeNumFeatures(_iter : Iterator[(Double, Array[Int], Array[Double])]): Iterator[Int] = {
    _iter.map { case (label, indices, values) =>
      indices.lastOption.getOrElse(0)
    }
  }

  def parseLibSVMIter( iter : Iterator[String]): Iterator[(Double, Array[Int], Array[Double])] = {
    iter
      .filter(line => !(line.isEmpty || line.startsWith("#")))
      .map(x => parseLibSVMRecord(x.trim))
  }

}

val supportedSparsities = Array(
  "maxSparsity",
  "minSparsity",
  "meanSparsity",
  "modeSparsity",
  "allSparsity",
  "distributeSparsites")
val strDS = input.map(_.getString(0))
val ard = strDS.mapPartitions(readLibSVM.parseLibSVMIter)
val numFeatures = ard.
  mapPartitions(readLibSVM.computeNumFeatures(_)).
  reduce((x,y) => scala.math.max(x,y))
val zeroArr = Array.fill[Int](numFeatures.toInt)(0)
// ard.rdd.treeAggregate(zeroArr)(seqOp,combOp)


sparsity = sparsityCategory match {
  case "maxSparsity" =>
    arrTypeData
      .mapPartitions(iter => iter.map(_._2.size))
      .reduce((x,y) => math.max(x,y)).toString
  case "minSparsity" =>
    arrTypeData
      .mapPartitions(iter => iter.map(_._2.size))
      .reduce((x,y) => math.min(x,y)).toString
  case "meanSparsity" =>
    arrTypeData
      .mapPartitions(iter => iter.map(_._2.size))
      .agg(Seq("value" -> "mean").toMap)
      .head
      .getDouble(0).toInt
      .toString
  case "modeSparsity" =>
    arrTypeData
      .mapPartitions(iter => iter.map(_._2.size))
      .groupBy("value")
      .count
      .agg(Seq("count" -> "max").toMap)
      .head
      .getLong(0)
      .toString
  case "allSparsity" =>
    val countSparsity = arrTypeData
        .mapPartitions(iter => iter.map(_._2.size))
    val minSparsity = countSparsity.reduce((x,y) => math.min(x,y))
    val maxSparsity = countSparsity.reduce((x,y) => math.max(x,y))
    val meanSparsity = countSparsity
        .agg(Seq("value" -> "mean").toMap)
        .head
        .getDouble(0).toInt
    val idxCount = countSparsity
        .groupBy("value")
        .count
    val modeCount = idxCount
        .agg(Seq("count" -> "max").toMap)
        .head
        .getLong(0)
    val modeSparsity = idxCount.where(s"count = ${modeCount}").head.getInt(0)
    Array(minSparsity,maxSparsity,meanSparsity,modeSparsity).mkString(",")
  case "distributeSparsites" =>
    val countSparsity = arrTypeData
        .mapPartitions(iter => iter.map(_._2.size))
    val idxCount = countSparsity
        .groupBy("value")
        .count
    val sortedSparsity = idxCount.rdd.map{
      line =>
        val idx = line.getInt(0)
        val count = line.getLong(1)
        val ratePercent = "%.2f%%".format(count.toDouble / data_count * 100d)
        (idx,count,ratePercent)
    }.sortBy(_._2,false).collect
    .mkString("\n")
  case _ => sparsityCategory
}

非法label个数
正负样本比、
非法feature个数（格式有错）
feature的index的最大值
特征的稀疏度
特征维度命中分布（可只统计出现过的）

paramMap("dataPath") = paramMap("parquetDataPath")
InputSparkLibsvm.init(paramMap.toMap)
val input = InputSparkLibsvm.read(spark).asInstanceOf[WeiDataFrame].get

checkIllegalLabel
rateNP
illegalFeatrues
featureNum
sparsityCategory
featureDistribution

^((-?\d+)(\.\d+)?)(( ((\d+):(-?\d+)(\.\d+)?)+)+)$
^((\d+):(-?\d+)(\.\d+)?)(( ((\d+):(-?\d+)(\.\d+)?)+)+)$
0 7:1.0 10:1.0 54:1.0 137:1.0 140:1.0 143:1.0 225:1.0 327:1.0 328:1.0 331:1.0 335:1.0 337:1.0 340:1.0 343:1.0 349:1.0 353:1.0 355:1.0 359:1.0 361:1.0 364:1.0 367:1.0 370:1.0 373:1.0 376:1.0 379:1.0 382:1.0 385:1.0 388:1.0 400:1.0 409:1.0 420:1.0 431:1.0 441:1.0 459:1.0 479:1.0 496:1.0 568:1.0 580:1.0 593:1.0 637:1.0 1639:1.0 2643:1.0 3646:1.0 4645:1.0 5647:1.0 6651:1.0 7654:1.0 8799:1.0 8803:1.0 8813:1.0 8824:1.0 8834:1.0 8861:1.0 8871:1.0 8896:1.0 8914:1.0 8937:1.0 8955:1.0 8971:1.0 8991:1.0 9009:1.0 9031:1.0 9049:1.0 9060:1.0 10062:1.0 11061:1.0 12088:1.0 12108:1.0 12109:1.0 13115:1.0 13136:1.0 13147:1.0 13158:1.0 13169:1.0 13170:1.0 14172:1.0 15174:1.0 16487:1.0 17466:1.0 18181:1.0








